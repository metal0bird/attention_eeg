{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model \n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOCUS_SOURCE_DIR = \"dataset_mtf/focus/\"\n",
    "UNFOCUS_SOURCE_DIR = \"dataset_mtf/unfocus/\"\n",
    "#DROWSY_SOURCE_DIR = \"dataset_2/drowsy/\"\n",
    "\n",
    "TRAINING_DIR = 'images_mtf/training/'\n",
    "VALIDATION_DIR = 'images_mtf/validation/'\n",
    "\n",
    "TRAINING_FOCUS_DIR = os.path.join(TRAINING_DIR, \"focus/\")\n",
    "VALIDATION_FOCUS_DIR = os.path.join(VALIDATION_DIR, \"focus/\")\n",
    "\n",
    "TRAINING_UNFOCUS_DIR = os.path.join(TRAINING_DIR, \"unfocus/\")\n",
    "VALIDATION_UNFOCUS_DIR = os.path.join(VALIDATION_DIR, \"unfocus/\")\n",
    "\n",
    "#TRAINING_DROWSY_DIR = os.path.join(TRAINING_DIR, \"drowsy/\")\n",
    "#VALIDATION_DROWSY_DIR = os.path.join(VALIDATION_DIR, \"drowsy/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original focus's directory has 12871 images\n",
      "Original unfocus's directory has 12870 images\n",
      "There are 9009 images of focus for training\n",
      "There are 9009 images of unfocus for training\n",
      "There are 3862 images of focus for validation\n",
      "There are 3861 images of unfocus for validation\n"
     ]
    }
   ],
   "source": [
    "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
    "print(f\"Original focus's directory has {len(os.listdir(FOCUS_SOURCE_DIR))} images\")\n",
    "print(f\"Original unfocus's directory has {len(os.listdir(UNFOCUS_SOURCE_DIR))} images\")\n",
    "#print(f\"Original drowsy's directory has {len(os.listdir(DROWSY_SOURCE_DIR))} images\\n\")\n",
    "\n",
    "# Training and validation splits\n",
    "print(f\"There are {len(os.listdir(TRAINING_FOCUS_DIR))} images of focus for training\")\n",
    "print(f\"There are {len(os.listdir(TRAINING_UNFOCUS_DIR))} images of unfocus for training\")\n",
    "#print(f\"There are {len(os.listdir(TRAINING_DROWSY_DIR))} images of drowsy for training\\n\")\n",
    "\n",
    "print(f\"There are {len(os.listdir(VALIDATION_FOCUS_DIR))} images of focus for validation\")\n",
    "print(f\"There are {len(os.listdir(VALIDATION_UNFOCUS_DIR))} images of unfocus for validation\")\n",
    "#print(f\"There are {len(os.listdir(VALIDATION_DROWSY_DIR))} images of drowsy for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# GRADED FUNCTION: train_val_generators\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "\n",
    "  train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow_from_directory method\n",
    "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
    "                                                      batch_size=20,\n",
    "                                                      class_mode='binary', #categorical\n",
    "                                                      target_size=(150, 150)\n",
    "                                                      )\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow_from_directory method\n",
    "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
    "                                                                batch_size=20,\n",
    "                                                                class_mode='binary', # categorical\n",
    "                                                                target_size=(150, 150)\n",
    "                                                                )\n",
    "  ### END CODE HERE\n",
    "  return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18017 images belonging to 2 classes.\n",
      "Found 7723 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(150,150,3),\n",
    "    classes=2,\n",
    "    classifier_activation='sigmoid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "outputs = keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.legacy.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "901/901 [==============================] - 159s 176ms/step - loss: 0.6324 - binary_accuracy: 0.6468 - val_loss: 0.5953 - val_binary_accuracy: 0.6778\n",
      "Epoch 2/30\n",
      "901/901 [==============================] - 194s 216ms/step - loss: 0.6074 - binary_accuracy: 0.6635 - val_loss: 0.5853 - val_binary_accuracy: 0.6828\n",
      "Epoch 3/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.6016 - binary_accuracy: 0.6698 - val_loss: 0.5991 - val_binary_accuracy: 0.6753\n",
      "Epoch 4/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.5994 - binary_accuracy: 0.6741 - val_loss: 0.5768 - val_binary_accuracy: 0.6988\n",
      "Epoch 5/30\n",
      "901/901 [==============================] - 197s 219ms/step - loss: 0.5963 - binary_accuracy: 0.6776 - val_loss: 0.5800 - val_binary_accuracy: 0.6956\n",
      "Epoch 6/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.5973 - binary_accuracy: 0.6765 - val_loss: 0.5758 - val_binary_accuracy: 0.7004\n",
      "Epoch 7/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.5963 - binary_accuracy: 0.6772 - val_loss: 0.5760 - val_binary_accuracy: 0.6964\n",
      "Epoch 8/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.5912 - binary_accuracy: 0.6831 - val_loss: 0.5713 - val_binary_accuracy: 0.6999\n",
      "Epoch 9/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.5936 - binary_accuracy: 0.6802 - val_loss: 0.5749 - val_binary_accuracy: 0.6984\n",
      "Epoch 10/30\n",
      "901/901 [==============================] - 197s 218ms/step - loss: 0.5910 - binary_accuracy: 0.6835 - val_loss: 0.5698 - val_binary_accuracy: 0.7018\n",
      "Epoch 11/30\n",
      "901/901 [==============================] - 186s 206ms/step - loss: 0.5962 - binary_accuracy: 0.6778 - val_loss: 0.5768 - val_binary_accuracy: 0.6898\n",
      "Epoch 12/30\n",
      "901/901 [==============================] - 160s 177ms/step - loss: 0.5887 - binary_accuracy: 0.6872 - val_loss: 0.5726 - val_binary_accuracy: 0.6958\n",
      "Epoch 13/30\n",
      "901/901 [==============================] - 159s 177ms/step - loss: 0.5942 - binary_accuracy: 0.6801 - val_loss: 0.5807 - val_binary_accuracy: 0.6935\n",
      "Epoch 14/30\n",
      "901/901 [==============================] - 159s 177ms/step - loss: 0.5934 - binary_accuracy: 0.6831 - val_loss: 0.5722 - val_binary_accuracy: 0.6971\n",
      "Epoch 15/30\n",
      "901/901 [==============================] - 160s 177ms/step - loss: 0.5859 - binary_accuracy: 0.6833 - val_loss: 0.5664 - val_binary_accuracy: 0.7043\n",
      "Epoch 16/30\n",
      "901/901 [==============================] - 160s 177ms/step - loss: 0.5929 - binary_accuracy: 0.6851 - val_loss: 0.6029 - val_binary_accuracy: 0.6756\n",
      "Epoch 17/30\n",
      "901/901 [==============================] - 159s 176ms/step - loss: 0.5963 - binary_accuracy: 0.6794 - val_loss: 0.5712 - val_binary_accuracy: 0.7005\n",
      "Epoch 18/30\n",
      "901/901 [==============================] - 158s 175ms/step - loss: 0.5934 - binary_accuracy: 0.6777 - val_loss: 0.5903 - val_binary_accuracy: 0.6860\n",
      "Epoch 19/30\n",
      "901/901 [==============================] - 157s 174ms/step - loss: 0.5920 - binary_accuracy: 0.6852 - val_loss: 0.5671 - val_binary_accuracy: 0.7058\n",
      "Epoch 20/30\n",
      "901/901 [==============================] - 159s 176ms/step - loss: 0.5870 - binary_accuracy: 0.6843 - val_loss: 0.5709 - val_binary_accuracy: 0.7022\n",
      "Epoch 21/30\n",
      "901/901 [==============================] - 158s 176ms/step - loss: 0.5899 - binary_accuracy: 0.6837 - val_loss: 0.5650 - val_binary_accuracy: 0.7054\n",
      "Epoch 22/30\n",
      "901/901 [==============================] - 158s 175ms/step - loss: 0.5919 - binary_accuracy: 0.6841 - val_loss: 0.6180 - val_binary_accuracy: 0.6522\n",
      "Epoch 23/30\n",
      "901/901 [==============================] - 160s 178ms/step - loss: 0.5913 - binary_accuracy: 0.6816 - val_loss: 0.5689 - val_binary_accuracy: 0.7019\n",
      "Epoch 24/30\n",
      "901/901 [==============================] - 157s 175ms/step - loss: 0.5916 - binary_accuracy: 0.6834 - val_loss: 0.5783 - val_binary_accuracy: 0.6973\n",
      "Epoch 25/30\n",
      "901/901 [==============================] - 157s 174ms/step - loss: 0.5902 - binary_accuracy: 0.6836 - val_loss: 0.5775 - val_binary_accuracy: 0.6986\n",
      "Epoch 26/30\n",
      "901/901 [==============================] - 157s 174ms/step - loss: 0.5884 - binary_accuracy: 0.6876 - val_loss: 0.5655 - val_binary_accuracy: 0.7056\n",
      "Epoch 27/30\n",
      "901/901 [==============================] - 157s 174ms/step - loss: 0.5938 - binary_accuracy: 0.6847 - val_loss: 0.5662 - val_binary_accuracy: 0.6982\n",
      "Epoch 28/30\n",
      "901/901 [==============================] - 158s 175ms/step - loss: 0.5920 - binary_accuracy: 0.6865 - val_loss: 0.6335 - val_binary_accuracy: 0.6466\n",
      "Epoch 29/30\n",
      "901/901 [==============================] - 157s 174ms/step - loss: 0.5925 - binary_accuracy: 0.6886 - val_loss: 0.5746 - val_binary_accuracy: 0.6925\n",
      "Epoch 30/30\n",
      "901/901 [==============================] - 157s 174ms/step - loss: 0.5950 - binary_accuracy: 0.6850 - val_loss: 0.5815 - val_binary_accuracy: 0.6892\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(train_generator, epochs=30, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#-----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Retrieve a list of list results on training and test data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sets for each training epoch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#-----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m acc\u001b[38;5;241m=\u001b[39m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m val_acc\u001b[38;5;241m=\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m=\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/step_1.keras\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "#model.compile(optimizer=keras.optimizers.legacy.Adam(1e-5),  # Very low learning rate\n",
    "#              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.legacy.Adam(1e-5),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
