{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/aman/Github bs/datasets/EEG Data/eeg_record7.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record6.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record4.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record5.mat', '/Users/aman/Github bs/datasets/EEG Data/.DS_Store', '/Users/aman/Github bs/datasets/EEG Data/eeg_record1.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record2.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record3.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record16.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record17.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record15.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record29.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record28.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record14.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record10.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record11.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record13.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record12.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record23.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record22.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record20.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record34.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record21.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record19.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record25.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record31.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record30.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record24.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record18.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record32.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record26.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record27.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record33.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record8.mat', '/Users/aman/Github bs/datasets/EEG Data/eeg_record9.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record7.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record6.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record4.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record5.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record1.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record2.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record3.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record16.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record17.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record15.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record29.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record28.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record14.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record10.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record11.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record13.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record12.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record23.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record22.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record20.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record34.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record21.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record19.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record25.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record31.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record30.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record24.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record18.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record32.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record26.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record27.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record33.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record8.mat', '/Users/aman/Github bs/datasets/EEG Data/EEG Data/eeg_record9.mat']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftshift\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "file_names=[]\n",
    "for dirname, _, filenames in os.walk('/Users/aman/Github bs/datasets/EEG Data/'):\n",
    "    for filename in filenames:\n",
    "        file_names.append(os.path.join(dirname, filename))\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        \n",
    "print(file_names)\n",
    "# each trial is about 54 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build 5 order high pass filter\n",
    "from scipy.signal import butter, lfilter, freqz   \n",
    "# ----- ----- ----- -----    \n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    x = signal.filtfilt(b, a, data)\n",
    "    y = signal.filtfilt(b, a, x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker=128*60*10\n",
    "#delete file #28 and 14 because it doesnot have enough data\n",
    "useful_file_index = [3,4,5,6,7,10,11,12,13,17,18,19,20,21,24,25,26,27,31,32,33,34]\n",
    "#useful_file_index = np.arange(1,35)\n",
    "useful_channels=[4,5,8,9,10,11,16]\n",
    "useful_channels_names=['F7','F3','P7','O1','O2','P8','AF4']\n",
    "chan_num=7\n",
    "trail_names=[]\n",
    "data_focus={}\n",
    "data_unfocus={}\n",
    "data_drowsy={}\n",
    "focus={}\n",
    "unfocus={}\n",
    "drowsy={}\n",
    "#for i in useful_file_index:\n",
    "i=1\n",
    "for index,filename in enumerate(filenames):\n",
    "    if int(filename.split('d')[1].split('.')[0]) in useful_file_index:\n",
    "        mat = scipy.io.loadmat(file_names[index])\n",
    "        trail_names.append(filename.split('.')[0])\n",
    "        data_focus[trail_names[-1]]=mat['o']['data'][0,0][0:marker,useful_channels].copy()\n",
    "        data_unfocus[trail_names[-1]]=mat['o']['data'][0,0][marker:2*marker,useful_channels].copy()\n",
    "        data_drowsy[trail_names[-1]]=mat['o']['data'][0,0][2*marker:3*marker,useful_channels].copy()\n",
    "        focus[trail_names[-1]]=mat['o']['data'][0,0][0:marker,useful_channels].copy()\n",
    "        unfocus[trail_names[-1]]=mat['o']['data'][0,0][marker:2*marker,useful_channels].copy()\n",
    "        drowsy[trail_names[-1]]=mat['o']['data'][0,0][2*marker:3*marker,useful_channels].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76800, 7)\n"
     ]
    }
   ],
   "source": [
    "type(data_focus['eeg_record3'])\n",
    "print(data_focus['eeg_record3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eeg_record7', 'eeg_record6', 'eeg_record4', 'eeg_record5', 'eeg_record3', 'eeg_record17', 'eeg_record10', 'eeg_record11', 'eeg_record13', 'eeg_record12', 'eeg_record20', 'eeg_record34', 'eeg_record21', 'eeg_record19', 'eeg_record25', 'eeg_record31', 'eeg_record24', 'eeg_record18', 'eeg_record32', 'eeg_record26', 'eeg_record27', 'eeg_record33']\n"
     ]
    }
   ],
   "source": [
    "print(trail_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Pass 0.16HZ\n",
    "row, col = data_focus['eeg_record3'].shape\n",
    "for name in trail_names:\n",
    "    for i in range(col):\n",
    "        data_focus[name][:,i]=butter_highpass_filter(data_focus[name][:,i], 0.16, 128, 5)\n",
    "        data_unfocus[name][:,i]=butter_highpass_filter(data_unfocus[name][:,i], 0.16, 128, 5)\n",
    "        data_drowsy[name][:,i]=butter_highpass_filter(data_drowsy[name][:,i], 0.16, 128, 5)\n",
    "        #print(name,data_drowsy[name][:,i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76800, 7)\n"
     ]
    }
   ],
   "source": [
    "type(data_focus['eeg_record3'])\n",
    "print(data_focus['eeg_record3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F7_0.5',\n",
       " 'F7_1.0',\n",
       " 'F7_1.5',\n",
       " 'F7_2.0',\n",
       " 'F7_2.5',\n",
       " 'F7_3.0',\n",
       " 'F7_3.5',\n",
       " 'F7_4.0',\n",
       " 'F7_4.5',\n",
       " 'F7_5.0',\n",
       " 'F7_5.5',\n",
       " 'F7_6.0',\n",
       " 'F7_6.5',\n",
       " 'F7_7.0',\n",
       " 'F7_7.5',\n",
       " 'F7_8.0',\n",
       " 'F7_8.5',\n",
       " 'F7_9.0',\n",
       " 'F7_9.5',\n",
       " 'F7_10.0',\n",
       " 'F7_10.5',\n",
       " 'F7_11.0',\n",
       " 'F7_11.5',\n",
       " 'F7_12.0',\n",
       " 'F7_12.5',\n",
       " 'F7_13.0',\n",
       " 'F7_13.5',\n",
       " 'F7_14.0',\n",
       " 'F7_14.5',\n",
       " 'F7_15.0',\n",
       " 'F7_15.5',\n",
       " 'F7_16.0',\n",
       " 'F7_16.5',\n",
       " 'F7_17.0',\n",
       " 'F7_17.5',\n",
       " 'F7_18.0',\n",
       " 'F3_0.5',\n",
       " 'F3_1.0',\n",
       " 'F3_1.5',\n",
       " 'F3_2.0',\n",
       " 'F3_2.5',\n",
       " 'F3_3.0',\n",
       " 'F3_3.5',\n",
       " 'F3_4.0',\n",
       " 'F3_4.5',\n",
       " 'F3_5.0',\n",
       " 'F3_5.5',\n",
       " 'F3_6.0',\n",
       " 'F3_6.5',\n",
       " 'F3_7.0',\n",
       " 'F3_7.5',\n",
       " 'F3_8.0',\n",
       " 'F3_8.5',\n",
       " 'F3_9.0',\n",
       " 'F3_9.5',\n",
       " 'F3_10.0',\n",
       " 'F3_10.5',\n",
       " 'F3_11.0',\n",
       " 'F3_11.5',\n",
       " 'F3_12.0',\n",
       " 'F3_12.5',\n",
       " 'F3_13.0',\n",
       " 'F3_13.5',\n",
       " 'F3_14.0',\n",
       " 'F3_14.5',\n",
       " 'F3_15.0',\n",
       " 'F3_15.5',\n",
       " 'F3_16.0',\n",
       " 'F3_16.5',\n",
       " 'F3_17.0',\n",
       " 'F3_17.5',\n",
       " 'F3_18.0',\n",
       " 'P7_0.5',\n",
       " 'P7_1.0',\n",
       " 'P7_1.5',\n",
       " 'P7_2.0',\n",
       " 'P7_2.5',\n",
       " 'P7_3.0',\n",
       " 'P7_3.5',\n",
       " 'P7_4.0',\n",
       " 'P7_4.5',\n",
       " 'P7_5.0',\n",
       " 'P7_5.5',\n",
       " 'P7_6.0',\n",
       " 'P7_6.5',\n",
       " 'P7_7.0',\n",
       " 'P7_7.5',\n",
       " 'P7_8.0',\n",
       " 'P7_8.5',\n",
       " 'P7_9.0',\n",
       " 'P7_9.5',\n",
       " 'P7_10.0',\n",
       " 'P7_10.5',\n",
       " 'P7_11.0',\n",
       " 'P7_11.5',\n",
       " 'P7_12.0',\n",
       " 'P7_12.5',\n",
       " 'P7_13.0',\n",
       " 'P7_13.5',\n",
       " 'P7_14.0',\n",
       " 'P7_14.5',\n",
       " 'P7_15.0',\n",
       " 'P7_15.5',\n",
       " 'P7_16.0',\n",
       " 'P7_16.5',\n",
       " 'P7_17.0',\n",
       " 'P7_17.5',\n",
       " 'P7_18.0',\n",
       " 'O1_0.5',\n",
       " 'O1_1.0',\n",
       " 'O1_1.5',\n",
       " 'O1_2.0',\n",
       " 'O1_2.5',\n",
       " 'O1_3.0',\n",
       " 'O1_3.5',\n",
       " 'O1_4.0',\n",
       " 'O1_4.5',\n",
       " 'O1_5.0',\n",
       " 'O1_5.5',\n",
       " 'O1_6.0',\n",
       " 'O1_6.5',\n",
       " 'O1_7.0',\n",
       " 'O1_7.5',\n",
       " 'O1_8.0',\n",
       " 'O1_8.5',\n",
       " 'O1_9.0',\n",
       " 'O1_9.5',\n",
       " 'O1_10.0',\n",
       " 'O1_10.5',\n",
       " 'O1_11.0',\n",
       " 'O1_11.5',\n",
       " 'O1_12.0',\n",
       " 'O1_12.5',\n",
       " 'O1_13.0',\n",
       " 'O1_13.5',\n",
       " 'O1_14.0',\n",
       " 'O1_14.5',\n",
       " 'O1_15.0',\n",
       " 'O1_15.5',\n",
       " 'O1_16.0',\n",
       " 'O1_16.5',\n",
       " 'O1_17.0',\n",
       " 'O1_17.5',\n",
       " 'O1_18.0',\n",
       " 'O2_0.5',\n",
       " 'O2_1.0',\n",
       " 'O2_1.5',\n",
       " 'O2_2.0',\n",
       " 'O2_2.5',\n",
       " 'O2_3.0',\n",
       " 'O2_3.5',\n",
       " 'O2_4.0',\n",
       " 'O2_4.5',\n",
       " 'O2_5.0',\n",
       " 'O2_5.5',\n",
       " 'O2_6.0',\n",
       " 'O2_6.5',\n",
       " 'O2_7.0',\n",
       " 'O2_7.5',\n",
       " 'O2_8.0',\n",
       " 'O2_8.5',\n",
       " 'O2_9.0',\n",
       " 'O2_9.5',\n",
       " 'O2_10.0',\n",
       " 'O2_10.5',\n",
       " 'O2_11.0',\n",
       " 'O2_11.5',\n",
       " 'O2_12.0',\n",
       " 'O2_12.5',\n",
       " 'O2_13.0',\n",
       " 'O2_13.5',\n",
       " 'O2_14.0',\n",
       " 'O2_14.5',\n",
       " 'O2_15.0',\n",
       " 'O2_15.5',\n",
       " 'O2_16.0',\n",
       " 'O2_16.5',\n",
       " 'O2_17.0',\n",
       " 'O2_17.5',\n",
       " 'O2_18.0',\n",
       " 'P8_0.5',\n",
       " 'P8_1.0',\n",
       " 'P8_1.5',\n",
       " 'P8_2.0',\n",
       " 'P8_2.5',\n",
       " 'P8_3.0',\n",
       " 'P8_3.5',\n",
       " 'P8_4.0',\n",
       " 'P8_4.5',\n",
       " 'P8_5.0',\n",
       " 'P8_5.5',\n",
       " 'P8_6.0',\n",
       " 'P8_6.5',\n",
       " 'P8_7.0',\n",
       " 'P8_7.5',\n",
       " 'P8_8.0',\n",
       " 'P8_8.5',\n",
       " 'P8_9.0',\n",
       " 'P8_9.5',\n",
       " 'P8_10.0',\n",
       " 'P8_10.5',\n",
       " 'P8_11.0',\n",
       " 'P8_11.5',\n",
       " 'P8_12.0',\n",
       " 'P8_12.5',\n",
       " 'P8_13.0',\n",
       " 'P8_13.5',\n",
       " 'P8_14.0',\n",
       " 'P8_14.5',\n",
       " 'P8_15.0',\n",
       " 'P8_15.5',\n",
       " 'P8_16.0',\n",
       " 'P8_16.5',\n",
       " 'P8_17.0',\n",
       " 'P8_17.5',\n",
       " 'P8_18.0',\n",
       " 'AF4_0.5',\n",
       " 'AF4_1.0',\n",
       " 'AF4_1.5',\n",
       " 'AF4_2.0',\n",
       " 'AF4_2.5',\n",
       " 'AF4_3.0',\n",
       " 'AF4_3.5',\n",
       " 'AF4_4.0',\n",
       " 'AF4_4.5',\n",
       " 'AF4_5.0',\n",
       " 'AF4_5.5',\n",
       " 'AF4_6.0',\n",
       " 'AF4_6.5',\n",
       " 'AF4_7.0',\n",
       " 'AF4_7.5',\n",
       " 'AF4_8.0',\n",
       " 'AF4_8.5',\n",
       " 'AF4_9.0',\n",
       " 'AF4_9.5',\n",
       " 'AF4_10.0',\n",
       " 'AF4_10.5',\n",
       " 'AF4_11.0',\n",
       " 'AF4_11.5',\n",
       " 'AF4_12.0',\n",
       " 'AF4_12.5',\n",
       " 'AF4_13.0',\n",
       " 'AF4_13.5',\n",
       " 'AF4_14.0',\n",
       " 'AF4_14.5',\n",
       " 'AF4_15.0',\n",
       " 'AF4_15.5',\n",
       " 'AF4_16.0',\n",
       " 'AF4_16.5',\n",
       " 'AF4_17.0',\n",
       " 'AF4_17.5',\n",
       " 'AF4_18.0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = []\n",
    "freq_range=np.arange(0.5,18.5,0.5)\n",
    "symb='_'\n",
    "#useful_channels_names=['F7','F3','P7','O1','O2','P8','AF4']\n",
    "for index,channel in enumerate(useful_channels_names):\n",
    "    for freq in freq_range:\n",
    "        feature_names.append(channel+symb+str(freq))\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT was then calculated at a time step of 1 s producing a set of time-varying DFT \n",
    "# amplitudes X STFT (t,Ï‰) at 1s intervals within each input EEG channel.\n",
    "t_win = np.arange(0,128)\n",
    "M = 128\n",
    "window_blackman=0.42-0.5*np.cos((2*np.pi*t_win)/(M-1))+0.08*np.cos((4*np.pi*t_win)/(M-1)) #window_blackman = signal.windows.blackmanharris(128)\n",
    "\n",
    "#col is 7\n",
    "power_focus = {}\n",
    "for name in trail_names:\n",
    "    power_focus[name]=np.zeros([col,513,601])\n",
    "    \n",
    "power_unfocus = {}\n",
    "for name in trail_names:\n",
    "    power_unfocus[name]=np.zeros([col,513,601])\n",
    "    \n",
    "power_drowsy = {}\n",
    "for name in trail_names:\n",
    "    power_drowsy[name]=np.zeros([col,513,601])\n",
    "\n",
    "#the output of the stft is 513*601,1 second data will produce 1 column of data,there are 601\n",
    "for name in trail_names:\n",
    "    for i in range(col):\n",
    "        f, t,y1=scipy.signal.stft(data_focus[name][:,i],fs=128, window=window_blackman, nperseg=128, \n",
    "                      noverlap=0, nfft=1024, detrend=False,return_onesided=True, boundary='zeros',\n",
    "                      padded=True)\n",
    "        f, t,y2=scipy.signal.stft(data_unfocus[name][:,i],fs=128, window=window_blackman, nperseg=128, \n",
    "                      noverlap=0, nfft=1024, detrend=False,return_onesided=True, boundary='zeros',\n",
    "                      padded=True)\n",
    "        f, t,y3=scipy.signal.stft(data_drowsy[name][:,i],fs=128, window=window_blackman, nperseg=128, \n",
    "                      noverlap=0, nfft=1024, detrend=False,return_onesided=True, boundary='zeros',\n",
    "                      padded=True)\n",
    "        power_focus[name][i,:,:]=(np.abs(y1))**2\n",
    "        power_unfocus[name][i,:,:]=(np.abs(y2))**2\n",
    "        power_drowsy[name][i,:,:]=(np.abs(y3))**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 513, 601)\n"
     ]
    }
   ],
   "source": [
    "type(power_drowsy['eeg_record3'])\n",
    "print(power_drowsy['eeg_record3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine bins into 0.5HZ, and keep 0-18 HZ.\n",
    "\n",
    "num=[]\n",
    "\n",
    "power_focus_bin = {}\n",
    "for name in trail_names:\n",
    "    power_focus_bin[name]=np.zeros([7,36,601])\n",
    "    \n",
    "power_unfocus_bin = {}\n",
    "for name in trail_names:\n",
    "    power_unfocus_bin[name]=np.zeros([7,36,601])\n",
    "    \n",
    "power_drowsy_bin = {}\n",
    "for name in trail_names:\n",
    "    power_drowsy_bin[name]=np.zeros([7,36,601])\n",
    "\n",
    "for name in trail_names:\n",
    "    for chn in range(col):\n",
    "        j=0\n",
    "        for i in range(1,144,4):\n",
    "            power_focus_bin[name][chn,j,:]=np.average(power_focus[name][chn,i:i+4,:],axis=0)\n",
    "            power_unfocus_bin[name][chn,j,:]=np.average(power_unfocus[name][chn,i:i+4,:],axis=0)\n",
    "            power_drowsy_bin[name][chn,j,:]=np.average(power_drowsy[name][chn,i:i+4,:],axis=0)\n",
    "            #print(np.average(power_drowsy[name][chn,i:i+4,:],axis=0).shape)\n",
    "            #if name=='eeg_record3':\n",
    "            #    if chn==0:\n",
    "            #        num.append((f[i:i+4]))\n",
    "            #    print(j)\n",
    "            j=j+1\n",
    "            \n",
    "#print(num)    \n",
    "#print(len(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 36, 601)\n"
     ]
    }
   ],
   "source": [
    "type(power_drowsy_bin['eeg_record3'])\n",
    "print(power_drowsy_bin['eeg_record3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avarage over 15 seconds running window.\n",
    "\n",
    "power_focus_ave = {}\n",
    "for name in trail_names:\n",
    "    power_focus_ave[name]=np.zeros([7,36,585])\n",
    "    \n",
    "power_unfocus_ave = {}\n",
    "for name in trail_names:\n",
    "    power_unfocus_ave[name]=np.zeros([7,36,585])\n",
    "    \n",
    "power_drowsy_ave = {}\n",
    "for name in trail_names:\n",
    "    power_drowsy_ave[name]=np.zeros([7,36,585])\n",
    "\n",
    "for name in trail_names:\n",
    "    for chn in range(col):\n",
    "        j=0\n",
    "        for k in range(0,585):\n",
    "            power_focus_ave[name][chn,:,j]=np.average(power_focus_bin[name][chn,:,k:k+15],axis=1)\n",
    "            power_unfocus_ave[name][chn,:,j]=np.average(power_unfocus_bin[name][chn,:,k:k+15],axis=1)\n",
    "            power_drowsy_ave[name][chn,:,j]=np.average(power_drowsy_bin[name][chn,:,k:k+15],axis=1)\n",
    "            #print(np.average(power_drowsy_bin[name][chn,:,k:k+15],axis=1).shape)\n",
    "            j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 36, 585)\n"
     ]
    }
   ],
   "source": [
    "type(power_drowsy_ave['eeg_record3'])\n",
    "print(power_drowsy_ave['eeg_record3'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the data into a vector \n",
    "#[252,585]\n",
    "\n",
    "svm_focus = {}\n",
    "for name in trail_names:\n",
    "    svm_focus[name]=np.zeros([252,585])\n",
    "    \n",
    "svm_unfocus = {}\n",
    "for name in trail_names:\n",
    "    svm_unfocus[name]=np.zeros([252,585])\n",
    "    \n",
    "svm_drowsy = {}\n",
    "for name in trail_names:\n",
    "    svm_drowsy[name]=np.zeros([252,585])\n",
    "\n",
    "for name in trail_names:\n",
    "    for j in range(585):      \n",
    "        svm_focus[name][:,j]=power_focus_ave[name][:,:,j].reshape(1,-1)\n",
    "        svm_unfocus[name][:,j]=power_unfocus_ave[name][:,:,j].reshape(1,-1)\n",
    "        svm_drowsy[name][:,j]=power_drowsy_ave[name][:,:,j].reshape(1,-1)\n",
    "    svm_focus[name]=10*np.log(svm_focus[name])\n",
    "    svm_unfocus[name]=10*np.log(svm_unfocus[name])\n",
    "    svm_drowsy[name]=10*np.log(svm_drowsy[name])\n",
    "# now, we get the svm vector 252*585 252 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 585)\n"
     ]
    }
   ],
   "source": [
    "type(svm_focus['eeg_record3'])\n",
    "print(svm_focus['eeg_record3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA first then SVC, labeling the data into 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focused -- 2\n",
    "unfocused -- 1\n",
    "drowsy -- 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------0\n",
    "label_focus = [0]*585\n",
    "#--------1\n",
    "label_unfocus = [1]*585\n",
    "#--------2\n",
    "label_drowsy = [2]*585\n",
    "\n",
    "#subject is the variable for all participants\n",
    "\n",
    "subj1_files={'eeg_record3','eeg_record4','eeg_record5','eeg_record6','eeg_record7'}\n",
    "subj2_files={'eeg_record10','eeg_record11','eeg_record12','eeg_record13'}\n",
    "subj3_files={'eeg_record17','eeg_record18','eeg_record19','eeg_record20','eeg_record21'}\n",
    "subj4_files={'eeg_record24','eeg_record25','eeg_record26','eeg_record27'}\n",
    "subj5_files={'eeg_record31','eeg_record32','eeg_record33','eeg_record34'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will try to use the data from all participants to train the model\n",
    "target=[]\n",
    "subj=np.array([]).reshape(252,0).copy()\n",
    "for name in trail_names:\n",
    "    subj=np.concatenate((subj,svm_focus[name]), axis=1)\n",
    "    subj=np.concatenate((subj,svm_unfocus[name]), axis=1)\n",
    "    subj=np.concatenate((subj,svm_drowsy[name]), axis=1)  \n",
    "    target = target+label_focus+label_unfocus+label_drowsy\n",
    "subj=subj.T\n",
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38610, 252)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the target: 38610\n",
      "the shape of the data from the subject1: (38610, 252)\n"
     ]
    }
   ],
   "source": [
    "print('length of the target:',len(target))\n",
    "print('the shape of the data from the subject1:', subj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part I only train the data for subject1\n",
    "target1=[]\n",
    "subj1=np.array([]).reshape(252,0).copy()\n",
    "for name in subj1_files:\n",
    "    subj1=np.concatenate((subj1,svm_focus[name]), axis=1)\n",
    "    subj1=np.concatenate((subj1,svm_unfocus[name]), axis=1)\n",
    "    subj1=np.concatenate((subj1,svm_drowsy[name]), axis=1)  \n",
    "    target1 = target1+label_focus+label_unfocus+label_drowsy\n",
    "subj1=subj1.T\n",
    "target1 = np.array(target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8775, 252)\n",
      "8775\n"
     ]
    }
   ],
   "source": [
    "print(subj1.shape)\n",
    "print(len(target1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for subject 2\n",
    "target2=[]\n",
    "subj2=np.array([]).reshape(252,0).copy()\n",
    "for name in subj2_files:\n",
    "    subj2=np.concatenate((subj2,svm_focus[name]), axis=1)\n",
    "    subj2=np.concatenate((subj2,svm_unfocus[name]), axis=1)\n",
    "    subj2=np.concatenate((subj2,svm_drowsy[name]), axis=1)  \n",
    "    target2 = target2+label_focus+label_unfocus+label_drowsy\n",
    "subj2=subj2.T\n",
    "target2 = np.array(target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Score for Training data with SVM Model for subject1: 0.9646723646723647\n",
      "Score of For Test data with SVM Model for subject1 : 0.9331908831908832\n"
     ]
    }
   ],
   "source": [
    "# Train the data from subject1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data_train, data_test, data_train_target, data_test_target = train_test_split(subj1, target1, test_size=0.8, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "X_train_scaled = scaler.transform(data_train)\n",
    "X_test_scaled = scaler.transform(data_test)\n",
    "\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train_scaled,data_train_target)\n",
    "print(f'The Score for Training data with SVM Model for subject1:',svc.score(X_train_scaled,data_train_target))\n",
    "print(f'Score of For Test data with SVM Model for subject1 : {svc.score(X_test_scaled,data_test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the data from all subjects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "training_score=[]\n",
    "testing_score=[]\n",
    "split_ratios=[0.6,0.7,0.8]\n",
    "for i in split_ratios:\n",
    "    data_train, data_test, data_train_target, data_test_target = train_test_split(subj, target, test_size=i, random_state=0)\n",
    "    scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "    X_train_scaled = scaler.transform(data_train)\n",
    "    X_test_scaled = scaler.transform(data_test)\n",
    "    split_ratios.append(i)\n",
    "    svc = SVC(kernel='rbf')\n",
    "    svc.fit(X_train_scaled,data_train_target)\n",
    "    training_score.append(svc.score(X_train_scaled,data_train_target))\n",
    "    testing_score.append(svc.score(X_test_scaled,data_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_accuracy(splits, train_accuracies, test_accuracies):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(splits, train_accuracies, marker='o', label='Training Accuracy')\n",
    "    plt.plot(splits, test_accuracies, marker='o', label='Testing Accuracy')\n",
    "    plt.xlabel('Splitting Ratio')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Testing Accuracy vs Splitting Ratio')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_test_accuracy(split_ratios, training_score, testing_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "var_explained=[]\n",
    "split_ratios=[0.6,0.7,0.8,0.9]\n",
    "for i in split_ratios:\n",
    "    data_train, data_test, data_train_target, data_test_target = train_test_split(subj, target, test_size=i, random_state=0)\n",
    "    scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "    X_train_scaled = scaler.transform(data_train)\n",
    "    X_test_scaled = scaler.transform(data_test)\n",
    "    #PCA should be used on scaled data\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    var = pca.explained_variance_/pca.explained_variance_.sum()\n",
    "    var_explained.append(var[0:20].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_accuracy(splits, var_explained):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(splits, var_explained, marker='o', label='Testing Accuracy')\n",
    "    plt.xlabel('Splitting Ratio')\n",
    "    plt.ylabel('Proportion of var explained')\n",
    "    plt.title('Training and Testing Accuracy vs Splitting Ratio')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_test_accuracy(split_ratios, var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#sns.heatmap(pca.components_)\n",
    "#print(sum(pca.components_[:,0]**2))\n",
    "\n",
    "pca_map=pd.DataFrame(pca.components_,columns=feature_names,index=np.arange(1,253))\n",
    "#pca.components_.shape\n",
    "pca_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE SVM linear model\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_scaled,data_train_target)\n",
    "print(f'The Score for Training data with SVM Linear Model for all subjects:',svc.score(X_train_scaled,data_train_target))\n",
    "print(f'Score of For Test data with SVM Linear Model for all subjects : {svc.score(X_test_scaled,data_test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE SVM linear model\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train_scaled,data_train_target)\n",
    "print(f'The Score for Training data with SVM Linear Model for all subjects:',svc.score(X_train_scaled,data_train_target))\n",
    "print(f'Score of For Test data with SVM Linear Model for all subjects : {svc.score(X_test_scaled,data_test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbor = KNeighborsClassifier(n_neighbors=3)\n",
    "neighbor.fit(X_train_scaled,data_train_target)\n",
    "print(\"the score for training with data from 5 participants:\",neighbor.score(X_train_scaled,data_train_target))\n",
    "print(\"the score for test data from 5 participants:\",neighbor.score(X_test_scaled,data_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=16)\n",
    "dt.fit(X_train_scaled,data_train_target)\n",
    "print(f\"the score for training with data from 5 participants:{dt.score(X_train_scaled,data_train_target)}\")\n",
    "print(\"the score for test data from 5 participants:\",dt.score(X_test_scaled,data_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for depth in range(1,35):\n",
    "    dt_reg = RandomForestClassifier(max_depth=depth,random_state=0)\n",
    "    dt_reg.fit(X_train_scaled,data_train_target)\n",
    "    train_scores = train_scores +[dt_reg.score(X_train_scaled,data_train_target)]\n",
    "    test_scores = test_scores + [dt_reg.score(X_test_scaled,data_test_target)]\n",
    "    \n",
    "x = list(range(1,35))\n",
    "plt.plot(x,train_scores,c='r',label='train')\n",
    "plt.plot(x,test_scores,c='b',label='test')\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Depth vs. Accuracy for Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train_pca[:,0:30],data_train_target)\n",
    "print(f'The Score for Training data with SVM RBF Model for all subjects:',svc.score(X_train_pca[:,0:30],data_train_target))\n",
    "print(f'Score of For Test data with SVM RBF Model for all subjects : {svc.score(X_test_pca[:,0:30],data_test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_pca[:,0:30],data_train_target)\n",
    "print(f'The Score for Training data with SVM RBF Model for all subjects:',svc.score(X_train_pca[:,0:30],data_train_target))\n",
    "print(f'Score of For Test data with SVM RBF Model for all subjects : {svc.score(X_test_pca[:,0:30],data_test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN after PCA\n",
    "neighbor.fit(X_train_pca[:,0:30],data_train_target)\n",
    "print(\"the score for training with data from 5 participants:\",neighbor.score(X_train_pca[:,0:30],data_train_target))\n",
    "print(\"the score for test data from 5 participants:\",neighbor.score(X_test_pca[:,0:30],data_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold validation\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "data_train_target=data_train_target\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "score_tr=[]\n",
    "score_cv=[]\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "\n",
    "for train,cv in kfold.split(X_train_scaled,data_train_target):\n",
    "    X_tr = X_train_scaled[train]\n",
    "    Y_tr = data_train_target[train]\n",
    "    X_cv = X_train_scaled[cv]\n",
    "    Y_cv = data_train_target[cv]\n",
    "    \n",
    "    tf.random.set_seed(0)\n",
    "    \n",
    "    svm.fit(X_tr,Y_tr)\n",
    "    score_tr.append(svm.score(X_tr,Y_tr))\n",
    "    score_cv.append(svm.score(X_cv,Y_cv))\n",
    "    \n",
    "    print(f'Score for {fold_no} Fold Training: {score_tr[-1]:.3f}')\n",
    "    print(f'Score for {fold_no} Fold cv    : {score_cv[-1]:.3f}')\n",
    "    print('----------------------------------')\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "print(f'Score of Average For Training: {np.mean(score_tr):.3f}')\n",
    "print(f'Score of Average For CV.     : {np.mean(score_cv):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
